{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amirhatamian/Statistical-Models-For-Data-Science/blob/main/Lesson3_27_11_2023_ToDo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRHHZekNADCu"
      },
      "source": [
        "# Write your own Google drive path to files\n",
        "DrivePath = \"/content/drive/My Drive/Colab Notebooks\"\n",
        "\n",
        "# Link to Google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vCkqHLCAHdb"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import statistics\n",
        "from scipy import stats\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Specific importing\n",
        "from pandas.plotting import lag_plot\n",
        "from statsmodels.tsa.stattools import acf\n",
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1. Time Series Data Visualization in Python - Part 2**\n",
        "\n",
        "##**1.1. Heatmaps**\n",
        "\n",
        "A heatmap is a graphical representation of data where each value of a matrix is represented as a color. It can be useful to easily visualise how the distribution of a value evolves over time, as well as to visualise correlation (and covariance) values in a given dataset."
      ],
      "metadata": {
        "id": "ypOn-lP6KRYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1 - Temperature data\n",
        "temperature_data = pd.read_csv(DrivePath +'/Data/Temperature.csv', na_values='',sep=';',header=None,names=['Date', 'Temp'], parse_dates=['Date'], index_col = 'Date')\n",
        "display(temperature_data)"
      ],
      "metadata": {
        "id": "zXfKbOGUKnOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install july   # For defining heatmaps with days and months\n",
        "import july # Similar package calplot\n",
        "july.heatmap(temperature_data.index, temperature_data['Temp'].values, cmap='jet', colorbar=True, title='Average temperatures: Boston',month_grid=True);\n",
        "# Note: if we add \"value_label=True\" we will visualise the value itself in each cell\n"
      ],
      "metadata": {
        "id": "dYWF5I_9MgQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dP-OkmuJonBA"
      },
      "outputs": [],
      "source": [
        "# Example 2 - Data on house sale prices for King County (America), for homes sold between May 2014-May 2015\n",
        "houses = pd.read_csv(DrivePath +'/Data/kc_house_data.csv', na_values='',sep=',', parse_dates=['date'])\n",
        "display(houses)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Heatmap to visualise the total number of houses sold per year/condition\n",
        "houses_year_cond = pd.pivot_table(houses, values = 'id',index='condition', columns='yr_built',aggfunc='count')\n",
        "display(houses_year_cond)\n",
        "sns.heatmap(houses_year_cond, cmap='hot');"
      ],
      "metadata": {
        "id": "gh0b1qU8V3dL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note - Another useful plot for relating variables, besides scatterplot, is given by relplot\n",
        "\n",
        "#fig, ax = plt.subplots(figsize=(10, 4));\n",
        "#scatter = plt.scatter(houses['bedrooms'], houses['price'], c=houses['bathrooms'], cmap='viridis')\n",
        "#legend1 = ax.legend(*scatter.legend_elements(), loc = 'center', bbox_to_anchor=(0.8, 0.5), title='Bathroom')\n",
        "#ax.add_artist(legend1);\n",
        "#ax.set(xlabel='Bedrooms', ylabel='Price',title='Scatter');\n",
        "\n",
        "sns.relplot(x='bedrooms',y='price',hue='bathrooms',height=4,data=houses);"
      ],
      "metadata": {
        "id": "auVtsEHIB2kV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1.2 Boxplot and Violin plots**\n",
        "\n",
        "Boxplots show the distribution of numeric data values. The box shows the quartiles of the dataset (Q1,Q3) while the whiskers extend to show the rest of the distribution (min, max), except for points that are determined to be outliers using the IQR (interquartile range) approach. \\\\\n",
        "Violin plots are similar to boxplots, except that they also show the probability density of the data at different values. As in the standard boxplots, they include a marker for the median of the data and a box indicating the interquartile range. Overlaid is a kernel density estimation. They are both used to compare a variable distribution (or sample distribution) across different \"categories\"."
      ],
      "metadata": {
        "id": "W0GXKdTq0S7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1 - Temperature data\n",
        "temp = pd.read_csv(DrivePath +'/Data/daily-min-temperatures.csv', na_values='', sep = ';', parse_dates= ['Date'], index_col='Date', dayfirst=True)\n",
        "temp['Period'] = temp.index.to_period('Y') # Add one colum to indicate the year (DatetimeIndex -> PeriodIndex)\n",
        "display(temp)\n"
      ],
      "metadata": {
        "id": "8r5F1DHxUktN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple time plot for visualising the min temperature information\n",
        "plt.figure(figsize=(10,3))\n",
        "plt.grid()\n",
        "plt.plot(temp['Temp'],'k-')\n",
        "plt.title('Temperature')\n",
        "plt.xlabel('Time [days]')\n",
        "plt.ylabel('Values');"
      ],
      "metadata": {
        "id": "1Im-HxsS6qtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Boxplot representing the temperature data for each year\n",
        "sns.boxplot(x=temp['Period'], y = temp['Temp'],palette='rainbow');"
      ],
      "metadata": {
        "id": "27ZS4jYTbpbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Boxplot representing the monthly temperature data for a specific year\n",
        "temp_year = temp.loc[:'1981-12-31',:]\n",
        "temp_year['Period'] = temp_year.index.to_period('M')\n",
        "display(temp_year)\n",
        "\n",
        "# Note: For Series type, we can use dt.strftime() function to do the conversion using specified date_format\n",
        "# For example, '%b' - Month name, %m - Month number\n",
        "sns.boxplot(x=temp_year['Period'].dt.strftime('%b'),y = temp_year['Temp'], palette='rainbow');\n"
      ],
      "metadata": {
        "id": "YNjXASRLXxJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Violin plot\n",
        "sns.violinplot(x=temp_year['Period'].dt.strftime('%b'),y = temp_year['Temp'], palette='rainbow');"
      ],
      "metadata": {
        "id": "87HSU2oAEqNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA8IVdCPoAOR"
      },
      "source": [
        "# Note: IQR for defining an outlier\n",
        "\n",
        "P = [10,300,450,470,550,350,320,1000]\n",
        "Q1_py = np.quantile(P,0.25)\n",
        "Q3_py = np.quantile(P,0.75)\n",
        "IQR = Q3_py-Q1_py\n",
        "Lower_Fence = Q1_py - 1.5*IQR\n",
        "Upper_Fence = Q3_py + 1.5*IQR\n",
        "\n",
        "T = []\n",
        "for i in P:\n",
        "    exp1 = i < Lower_Fence\n",
        "    exp2 = i > Upper_Fence\n",
        "    if exp1 or exp2:\n",
        "       temp = i\n",
        "       T.append(temp)\n",
        "\n",
        "print('Outliers:', T)\n",
        "\n",
        "plt.figure(figsize=(3,4))\n",
        "ax = sns.boxplot(y = P, palette='rainbow');\n",
        "ax.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SD6BIufVvdtg"
      },
      "source": [
        "When the distribution of values in the sample is Gaussian or Gaussian-like, the standard deviation of the sample can be used as a cut-off for identifying outliers. In particular, 3*standard deviations from the mean will account for the 99.7% of the sample data (2 * standard deviations 95%)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBqgtAk2xBiM"
      },
      "source": [
        "# Standard deviation approach for defining outliers\n",
        "np.random.seed(42)\n",
        "P = 5 * np.random.randn(10000) + 50\n",
        "\n",
        "mean_value = np.mean(P)\n",
        "sd_value = np.std(P)\n",
        "thr = 3*sd_value\n",
        "Lower = mean_value - thr\n",
        "Upper = mean_value + thr\n",
        "\n",
        "outliers = [i for i in P if i < Lower or i > Upper]\n",
        "print('Number of outliers:', len(outliers))\n",
        "\n",
        "P_no_outliers = [i for i in P if i > Lower and i < Upper]\n",
        "print('Number of non-outliers:', len(P_no_outliers))\n",
        "\n",
        "plt.figure(figsize=(3,4))\n",
        "ax = sns.boxplot(y = P, palette='rainbow');\n",
        "ax.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1.3 Barplots and Histograms**\n",
        "A barplot visualizes data with rectangular bars with height proportional to the values that they represent. There are different types of barplots, according to the information they convey:\n",
        "> - Simple Bar Plot: an item for each category is shown by plotting bars of equal width but variable length;\n",
        "\n",
        "> - Grouped Bar Plot: the bars for categorical variable values are very close to each other, and hence the name. This type of plot is used for plotting a set of entities split in groups and subgroups;\n",
        "\n",
        "> - Stacked Bar Plot: bars represent the sub-groups, and are placed on top of each other to form a single column (or say, single bar). The overall length of the bar gives the total size of the category, and different colors indicate their relative contribution to each sub-group.\n"
      ],
      "metadata": {
        "id": "c3qZz4lxolNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1 - Temperature data (Yearly and Monthly)\n",
        "# Barplot\n",
        "temp = pd.read_csv(DrivePath +'/Data/daily-min-temperatures.csv', na_values='', sep = ';', parse_dates= ['Date'], index_col='Date', dayfirst=True)\n",
        "temp['Period'] = temp.index.to_period('Y') # Add one colum to indicate the year (DatetimeIndex -> PeriodIndex)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "sns.barplot(ax = axes[0], x=temp['Period'].dt.strftime('%Y'), y= temp['Temp'],palette='autumn',errorbar=('se'),capsize=0.01,estimator='mean')\n",
        "axes[0].set_title('Yearly Data', fontsize=12)\n",
        "axes[0].grid()\n",
        "\n",
        "sns.barplot(ax = axes[1], x=temp_year['Period'].dt.strftime('%b'), y= temp_year['Temp'],palette='autumn',errorbar=('se'),capsize=0.01,estimator='mean')\n",
        "axes[1].set_title('Monthly Data - 1981', fontsize=12)\n",
        "axes[1].grid();\n"
      ],
      "metadata": {
        "id": "QzTxWE4PjG3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple histograms can represent a good first step for understanding a dataset, allowing to show the frequency of numerical data. The `hist()` function requires only a single argument, that is an array of elements. It can return:\n",
        "> - *n*: heights of the histogram bins \\\\\n",
        "> - *bins*: edges of the binsâ€™ base \\\\\n",
        "> - *patches*: containers of individual artists used to create the histogram \\\\\n",
        "\n",
        "Among the optional arguments important are bins (i.e., how many bins to use to divide data) and density (if True, returns a probability density. Each bin will thus display the bins count divided by the total number of counts and the bin width. In this way, area under the histogram integrates to 1)."
      ],
      "metadata": {
        "id": "1bojasiKpxsu"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpLDFu52oBTN"
      },
      "source": [
        "# Histogram - Counts/Frequency (Daily data for 1981)\n",
        "n, bins, patches = plt.hist(temp_year['Temp'], bins = 20, density=False) # n = the number of points in a given bin\n",
        "plt.grid()\n",
        "print('The total counts is', sum(n)) # sum of the bins heights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Histogram -  Density\n",
        "n, bins, patches = plt.hist(temp_year['Temp'], bins = 20, density=True) # n = count_bin/(width_bin*total_counts)\n",
        "plt.grid()\n",
        "a = np.diff(bins) # to get the width of each bin\n",
        "print('The total area is', sum(n*a[0])) # sum of the bins' areas"
      ],
      "metadata": {
        "id": "4xl_OWi-nsN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3Gbn53JoBTO"
      },
      "source": [
        "A histogram aims to approximate the underlying probability density function that generated the data by binning and counting observations. Kernel density estimation (KDE) presents a different solution, where rather than using discrete bins, it smooths the observations with a Gaussian kernel, producing a continuous density estimate:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ju9fY--oBTO"
      },
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(12, 3))\n",
        "sns.histplot(temp_year['Temp'],kde=True,color='red',alpha=0.2,stat='probability',binwidth=1,fill=True, ax=axes[0]);\n",
        "axes[0].grid()\n",
        "axes[0].set_xlim([0, 25]);\n",
        "axes[0].set_ylim([0, 0.09]);\n",
        "\n",
        "sns.kdeplot(temp_year['Temp'],color='r', shade=True, ax=axes[1])\n",
        "axes[1].grid()\n",
        "axes[1].set_xlim([0, 25]);\n",
        "axes[1].set_ylim([0, 0.09]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Exercise 1 - Temperature & CO2**\n",
        "\n",
        "Consider the two csv files named \"CO2.csv\" and \"GLB.Ts+dSST.csv\". The *CO2.csv* file contains the corresponding data from the US National Oceanic and Atmospheric Administration, and *Trend* is the variable we are interested in (note: be careful with the date information). *GLB.Ts+dSST.csv* instead includes the global temperature anomalies (combined land-surface air and sea-surface water temperature anomalies - land-ocean temperature index, L-OTI) expressed as deviations from the corresponding 1951-1980 means (https://data.giss.nasa.gov/gistemp/). \\\\\n",
        "After having loaded the data and explored the datasets, convert the data in order to have yearly frequency (average as aggregator). Perform all the necessary operations required to calculate and then visualise in an appropriate graph the correlation between the yearly time courses of CO2 and global temperature anomalies."
      ],
      "metadata": {
        "id": "Sp1sFLHjM9K1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Exercise 2 - Brain Activity and Correlation**\n",
        "\n",
        "Consider the *Ab_pASL_Yeo_Average.txt* file which contains the brain time courses for 100 different regions and 200 time points.\n",
        "After having loaded the data, perform the following operations: \\\\\n",
        "1. Visualise in the same figure the signals for regions 3 and 4;\n",
        "2. Subtract the temporal mean from signal 3 (and the same for signal 4), and plot together in a new figure the demeaned signals;\n",
        "3. Calculate the correlation coefficient for each pair of time courses, and then the covariance. What is the resulting dimension of these two operations?\n",
        "4. Visualize the two results from step 3 using the most appropriate graph. In addition, in another graph, visualize the distribution of the correlation values just calculated;\n",
        "5. Calculate the correlation between the correlation coefficient values and the covariance ones.  "
      ],
      "metadata": {
        "id": "2r61P1sjEvUB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1.4 Lag plots**\n",
        "\n",
        "A lag plot checks whether a time series is random or not. Random data should not exhibit any identifiable structure in the lag plot, while non-random structure in the lag plot indicates that the underlying data are correlated (not random).\n",
        "A lag is a fixed time displacement. A plot of lag 1 is a plot of the values of Yi versus Yi+1, thus a lag plot is essentially a scatter plot with the two time series properly lagged."
      ],
      "metadata": {
        "id": "am3UB_pGXHhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example with yearly data on Australian beer production\n",
        "df = pd.read_csv(DrivePath +'/Data/Australian_Beer_production.csv', na_values='',sep=';', parse_dates=['time'],index_col='time')\n",
        "signal = df['value']\n",
        "plt.plot(signal,marker='.')\n",
        "plt.grid()\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Values');\n"
      ],
      "metadata": {
        "id": "YlM9W9VgXHhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lagged version of the original signal\n",
        "mysignals = [{'name': 'Lag0', 'y': signal,'color':'g', 'linewidth':2},\n",
        "              {'name': 'Lag1', 'y': signal.shift(1),'color':'r', 'linewidth':2},\n",
        "            {'name': 'Lag2', 'y':  signal.shift(2),'color':'b', 'linewidth':2}]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6,5))\n",
        "for line_lag in mysignals:\n",
        "    ax.plot(line_lag['y'],\n",
        "            color=line_lag['color'],\n",
        "            linewidth=line_lag['linewidth'],\n",
        "            label=line_lag['name'])\n",
        "\n",
        "ax.grid()\n",
        "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "ax.set_title('Representative Lags');"
      ],
      "metadata": {
        "id": "SivEXXq1XHhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lag plots\n",
        "fig, axes = plt.subplots(3,3, sharex=True, sharey=True, figsize=(8,8))\n",
        "\n",
        "for i, ax in enumerate(axes.flatten()[:9]):\n",
        "    pd.plotting.lag_plot(signal, lag=i+1, ax=ax, c='black')\n",
        "    ax.grid()\n",
        "    ax.set_xlabel('y(t)')\n",
        "    ax.set_ylabel('y(t+'+str(i+1)+')')\n",
        "    pt = (350, 350)\n",
        "    ax.axline(pt, slope=1, color='gray')\n",
        "\n"
      ],
      "metadata": {
        "id": "i57E3zQUXHhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1.4.1 Autocorrelation**\n",
        "\n",
        "Just as correlation correlation measures the extent of a linear relationship between two variables, autocorrelation measures the linear relationship between lagged values of a time series. There are several autocorrelation coefficients, corresponding to each panel in the lag plot. For example, r1 measures the relationship between yt and yt+1, r2 measures the relationship between y and yt+2, and so on.\n",
        "The autocorrelation coefficients make up the *autocorrelation function* or *ACF*."
      ],
      "metadata": {
        "id": "rgXMlzp6l4hZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1 - Simulated Signal\n",
        "a = pd.Series([10, 15, 30, 40, 60])\n",
        "b = a.shift(1)[1:]\n",
        "display(a)\n",
        "display(b)\n"
      ],
      "metadata": {
        "id": "iTi_cmkjsm3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# General simple equation to calculate the autocorrelation\n",
        "a2 = a[1:]-np.mean(a)\n",
        "b2 = b-np.mean(a)\n",
        "den = a-np.mean(a)\n",
        "res = sum(a2*b2)/sum(den**2)\n",
        "display(res)\n",
        "\n",
        "# Double check with automatic Python function:\n",
        "val = acf(a,nlags=4)\n",
        "display(val)"
      ],
      "metadata": {
        "id": "1p2eD9wrtIl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Automatic plot of the ACF function for the previous signal (Australian beer production)\n",
        "fig, ax = plt.subplots(figsize=(8,3))\n",
        "plot_acf(signal, lags=9,ax=ax)\n",
        "ax.grid()\n",
        "\n",
        "# Note: to retrieve the raw values resulting from this operation, we can use acf():\n",
        "val = acf(signal,nlags=9)\n",
        "display(val)\n",
        "\n",
        "# val1 indicates how successive values of the signals relate to each other\n",
        "# val2 indicates how signal values 2 period aparts relate to each other"
      ],
      "metadata": {
        "id": "ZbUFtxhbmodh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This plot is often known as correlogram. The shaded indicate whether the correlations are significantly different from zero and so if there is evidence of no autocorrelation structure."
      ],
      "metadata": {
        "id": "Io1vdNPXlDKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1.4.2 White Noise**\n",
        "\n",
        "Time series that show no autocorrelation are called **white noise**. White noise data is uncorrelated across time with zero mean and constant variance.As such, white noise variations in the data cannot be explained by any model. We should regard this as something out of interest and not predictable. \\\\\n",
        "When looking at the autocorrelation results for different lags, all the values should be close to zero."
      ],
      "metadata": {
        "id": "5mWIfZUttEyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random signal and Lag plot\n",
        "np.random.seed(42)\n",
        "\n",
        "mean = 0\n",
        "std = 1\n",
        "num_samples = 1000\n",
        "samples = pd.Series(np.random.normal(mean, std, size=num_samples))\n",
        "\n",
        "fig, axes = plt.subplots(figsize=(3,3))\n",
        "plt.plot(samples)\n",
        "plt.grid()\n",
        "plt.title('Random signal')\n",
        "\n",
        "fig, axes = plt.subplots(3,3, sharex=True, sharey=True, figsize=(8,8))\n",
        "\n",
        "for i, ax in enumerate(axes.flatten()[:9]):\n",
        "    pd.plotting.lag_plot(samples, lag=i+1, ax=ax, c='black')\n",
        "    ax.grid()\n",
        "    ax.set_xlabel('y(t)')\n",
        "    ax.set_ylabel('y(t+'+str(i+1)+')')\n",
        "    pt = (-4, -4)\n",
        "    ax.axline(pt, slope=1, color='gray')"
      ],
      "metadata": {
        "id": "foofRRE5XHht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Autocorrelation of the random signal\n",
        "fig, ax = plt.subplots(figsize=(8,4))\n",
        "plot_acf(samples, lags=9,ax=ax)\n",
        "ax.grid()\n",
        "\n",
        "val = acf(samples,nlags=9)\n",
        "display(val)"
      ],
      "metadata": {
        "id": "hs5We-Zl7zJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**(Short) Exercise 3 - Google Stock Price**\n",
        "After having loaded the Google Stock Prices from beginning 2004 till the end of 2016 with monthly frequency, plot the data and the autocorrelation plot with the corresponding  values (note: consider as maximum value for the autocorrelation lags within one year). How does it look like? \\\\\n",
        "Calculate then the differencing with the previous month. What happens to signal and to the the autocorrelation values?\n",
        "\n"
      ],
      "metadata": {
        "id": "3_LFSdpEv5aF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Exercise Extra  - Energy**\n",
        "\n",
        "In this exercise, load the data contained in the \"Energy_consumption.csv\" file. This contains information related to the Germany country-wide totals of electricity consumption, wind power production, and solar power production (wind+solar is also reported)."
      ],
      "metadata": {
        "id": "N4xZ9ytPOs6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data, dropping all the rows with NaN values and the Wind+Solar column\n"
      ],
      "metadata": {
        "id": "0A6C_0LRLVwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resample the dates with a frequency of 15 days, taking mean as aggregator, and visualise the time course of the total Consumption over years\n"
      ],
      "metadata": {
        "id": "L6CsPk3kMOWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare a figure with three subplots. In each subplot, illustrate the distribution of the total Consumption, Wind and Solar production values, respectively,\n",
        "# grouped by year\n"
      ],
      "metadata": {
        "id": "KPfRNSy-OEvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Represent with an appropriate graph the correlation between Wind and Consumption time courses\n"
      ],
      "metadata": {
        "id": "qfwLyYry3pnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate two matrices (dimension=3x3), one representing the Pearson correlation coefficient values for each pair of signals\n",
        "# and the other one the corresponding p-values. Visualise both of them with appropriate graphs\n"
      ],
      "metadata": {
        "id": "u4lJT9W63gNM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}