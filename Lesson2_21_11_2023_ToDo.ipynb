{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amirhatamian/Statistical-Models-For-Data-Science/blob/main/Lesson2_21_11_2023_ToDo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XroFBnnv9KsV"
      },
      "source": [
        "# Write your own Google drive path to files\n",
        "DrivePath = \"/content/drive/My Drive/Colab Notebooks\"\n",
        "\n",
        "# Link to Google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import statistics\n",
        "from scipy import stats\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn; seaborn.set()"
      ],
      "metadata": {
        "id": "Bd9rraZpl9FB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xchVBZ0aLU7n"
      },
      "source": [
        "#**1. Working with Time Series - Dates and Times**\n",
        "Every observation in a time series has an associated date or time. As such, we need specific objects for storing and working with dates and related measures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyrvtW6iNDOm"
      },
      "source": [
        "##**1.1 Dates and Times - Native Python**\n",
        "\n",
        "The basic objects for working with dates and times can be created using the built-in `datetime` module.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6DBcdkSa0VY"
      },
      "source": [
        "from datetime import datetime # Provides classes for manipulating dates and times\n",
        "from dateutil import parser"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7W8AB01lfugG"
      },
      "source": [
        "# To manually build a date, specifying the different inputs\n",
        "date = datetime(year=2021, month=11, day=29,hour=11, minute=30, second=25) # the last three are optional, default is 00:00:00\n",
        "print(date)\n",
        "print(type(date))\n",
        "\n",
        "# To parse a string into a datetime object  (strptime)\n",
        "xmas_day = '2021-12-25'\n",
        "print(datetime.strptime(xmas_day, '%Y-%m-%d'))\n",
        "\n",
        "\n",
        "# Note: the dateutil module provides the parser.parse function that can automatically parse dates from a variety of string formats:\n",
        "\n",
        "first_day = '1st of January, 2022'\n",
        "last_day = '31/12/21'\n",
        "random_day = 'Nov 08, 1999 10:32 AM'\n",
        "random_day2 = '20180803213450'\n",
        "\n",
        "print(parser.parse(first_day))\n",
        "print(parser.parse(last_day))\n",
        "print(parser.parse(random_day))\n",
        "print(parser.parse(random_day2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tid-_59imJHe"
      },
      "source": [
        "##**1.2 Dates and Times - Pandas**\n",
        "\n",
        "Pandas was developed in the context of financial modeling, thus it contains several tools for working with dates, times, and time-indexed data. It provides three important data structures for working with these data types:\n",
        "\n",
        "\n",
        "*   *Timestamp*: this allows working with time stamps that are particular moment in time (e.g., October 31th, 2010 at 8am). This is a replacement for Python native `datetime`, as it is based on a more efficient numpy.datetime64 type. Pandas represents timestamps using instances of `Timestamp` and sequences of timestamps using instances of `DatetimeIndex`;\n",
        "*   *Period*: this allows working with periods, i.e. intervals of time (e.g., 24 hour-long period). It is useful for example to check whether a specific event occurs within a certain period. The associated index structure is *PeriodIndex*;\n",
        "*  *Timedelta*: this allows working with time deltas (or durations, e.g., a duration of 3 minutes or 45 seconds).\n",
        "The associated index structure is *TimedeltaIndex*.\n",
        "\n",
        "\n",
        "Note: it is essential to save the index of a DataFrame as a DatetimeIndex and not as strings!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCJYBs0XQBKe"
      },
      "source": [
        "###**1.2.1. Timestamp and DatetimeIndex**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxRJtRKSmHaS"
      },
      "source": [
        " # Creating a timestamp object - Example 1\n",
        "xmas_day = pd.to_datetime('25th of Dec, 2021') # an alternative: pd.to_datetime('12/25/21')\n",
        "print(xmas_day)\n",
        "print(type(xmas_day)) # Timestamp type\n",
        "\n",
        "# If I want to convert to a string:\n",
        "Year = xmas_day.strftime('%Y')\n",
        "print('Year:', Year)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGVgm_yRnpuE"
      },
      "source": [
        "# When passing a series of dates, pd.to_datetime() returns a DatetimeIndex, i.e. a group of Timestamp objects\n",
        "dates = pd.to_datetime([datetime(2020, 12, 25), '4th of July, 2020',\n",
        "                       '2018-Oct-21', '20200508', '1982/1/22'])\n",
        "print(dates)\n",
        "print(type(dates))\n",
        "print('-----')\n",
        "\n",
        "# Alternative way to create a DatetimeIndex\n",
        "D = pd.DatetimeIndex(['2014-07-04', '2014-08-04',\n",
        "                          '2015-07-04', '2015-07-22'])\n",
        "print(D)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-cAMuhzrg-I"
      },
      "source": [
        "DatetimeIndex are very useful as we can use them as index for the Series objects:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xcnLrdGrfmC"
      },
      "source": [
        "index = pd.DatetimeIndex(['2014-07-04', '2014-03-04', '2015/07/22',\n",
        "                          '7/4/99', '01/01/1900'])\n",
        "data = pd.Series([10, 21, 32, 43,99], index=index)\n",
        "print(index)\n",
        "print('-----')\n",
        "print(data)\n",
        "\n",
        "print('-----')\n",
        "print('Single element:', data.iloc[0]) # or data[0]\n",
        "print('Single element with explicit indexing:', data.loc['1999-07-04'])\n",
        "print('Specific year: \\n', data['2014'])\n",
        "print('Specific year/month: \\n', data['2015-07'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGjTMTgjQM8_"
      },
      "source": [
        "###**1.2.2. Period and PeriodIndex**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx_G2byZwrDk"
      },
      "source": [
        "# To convert a DatetimeIndex to a Period object, frequency has to be specified\n",
        "dates = pd.to_datetime(['2013-02-02', '2012-01-02', '2015-11-30'])\n",
        "print(dates) # DatetimeIndex object\n",
        "print('-------------')\n",
        "\n",
        "period_daily = dates.to_period('D') # create daily time periods, PeriodIndex object\n",
        "print('Day:', period_daily)\n",
        "print('-------------')\n",
        "period_weekly = dates.to_period('W') # create weekly time periods\n",
        "print('Week:', period_weekly)\n",
        "print('-------------')\n",
        "period_monthly = dates.to_period('M') # create monthly time periods\n",
        "print('Month:', period_monthly)\n",
        "print('-------------')\n",
        "period_yearly = dates.to_period('Y') # create yearly time periods\n",
        "print('Year:', period_yearly)\n",
        "\n",
        "# Start/end time of a Period or other operations can be done on these PeriodIndex objects\n",
        "Stime = period_weekly.start_time # becomes a DatetimeIndex object\n",
        "Etime = period_monthly.end_time\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oh4r7_xKQnAq"
      },
      "source": [
        "###**1.2.3 Timedelta and TimedeltaIndex**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4Kb1yfKQwE8"
      },
      "source": [
        "# A TimedeltaIndex is given by the temporal difference between a DatetimeIndex and Timestamp objects\n",
        "dates = pd.to_datetime([datetime(2020, 12, 25), '31st of December, 1990',\n",
        "                       '2018-Oct-6', '07-07-2017', '20200508', '20200422T203448']) #DatetimeIndex\n",
        "dates_v2 = pd.to_datetime('2019-09-15') # Timestamp\n",
        "\n",
        "Difference = dates-dates_v2\n",
        "print(Difference)\n",
        "print('----------')\n",
        "Difference_2 = dates[2] - dates_v2\n",
        "print(Difference_2)\n",
        "print(type(Difference_2))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKNQYh4tV8N6"
      },
      "source": [
        "##**1.3 Creating Date Sequences**\n",
        "\n",
        "Regular date sequences can be automatically created using functions, such as `pd.date_range()` for sequences of timestamps, `pd.period_range()` for periods, and `pd.timedelta_range()` for time deltas.\n",
        "Frequency can also be changed accordingly in order to create something more precise, depending on our purpouses, as we will see below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHd1PP7va5EN"
      },
      "source": [
        "###**1.3.1 Sequence of dates: `pd.date_range()`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm3pfmjCZLCF"
      },
      "source": [
        "# 1. Simple sequence of Dates by specifying start/end: by default, the frequency is daily (output type is DatetimeIndex)\n",
        "day = pd.date_range('2021-08-01', '2021-08-10', freq='B') # B for business day only\n",
        "week = pd.date_range('2021-08-01', '2021-08-10',freq='W') # W weekly frequency\n",
        "month = pd.date_range('2021-08-01', '2021-10-31',freq='M') # M monthly frequency\n",
        "\n",
        "print(day)\n",
        "print(week)\n",
        "print(month)\n",
        "\n",
        "# Note: a complete list of frequencies that can be used is provided at this link: https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4A9s8ZgaGm4"
      },
      "source": [
        "# 2. Sequence of Dates by specifying Period: date range is specified with a start point and number of periods (to set the number of samples)\n",
        "date_rng_d = pd.date_range('01-01-1900 10:15', periods=5) # default daily frequency\n",
        "print(date_rng_d)\n",
        "\n",
        "date_rng_m = pd.date_range('01-01-1900 10:15', periods=5, freq='M') #M = last day of the month\n",
        "print(date_rng_m)\n",
        "\n",
        "date_rng_ms = pd.date_range('01-01-1900 10:15', periods=5, freq='MS') # MS = month start\n",
        "print(date_rng_ms)\n",
        "\n",
        "test = pd.date_range('2020-02-03 10:15', periods=8, freq = 'W')\n",
        "print(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVNqzQeT6SEY"
      },
      "source": [
        "# Note: date_range() output can be used as index in a Series object or in a DataFrame\n",
        "# Series example\n",
        "rng = pd.date_range('2016 Jul 1', periods = 10, freq = 'D')\n",
        "rng\n",
        "Series1 = pd.Series(list(range(len(rng))), index = rng)\n",
        "print(Series1)\n",
        "print('-------')\n",
        "\n",
        "# DataFrame example\n",
        "np.random.seed(42)\n",
        "DF1 = pd.DataFrame(60*np.random.rand(10,1), # random samples from a uniform distribution over [0, 1), the dimension of the output has to be specified\n",
        "             columns=['Values'], index=rng)\n",
        "DF1\n",
        "\n",
        "# As alternative, we can have our default values for index, and then set the index to the rng values\n",
        "# DF1 = pd.DataFrame(60*np.random.rand(10,1),\n",
        "#             columns=['Values'])\n",
        "# DF1.set_index(rng, inplace=True)\n",
        "# DF1.index # DatetimeIndex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwE2IWs7bxzi"
      },
      "source": [
        "###**1.3.2 Sequence of periods: `pd.period_range()`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mcwZOIJb9ST"
      },
      "source": [
        "# Create a period (interval) specifying the starting point and the number of periods\n",
        "A = pd.period_range('2020-02-03', periods=8, freq='D')\n",
        "print('A (Year):',A)\n",
        "print('-----------')\n",
        "B = pd.period_range('2020-02-03', periods=8, freq='M')\n",
        "print('B (Month):',B)\n",
        "print('-----------')\n",
        "C = pd.period_range('2020-02-03', periods=8, freq='W')\n",
        "print('C (Week):',C)\n",
        "print('-----------')\n",
        "D = pd.period_range('2020-02-03', periods=8, freq='10H')\n",
        "print('D (10 Hours):',D)\n",
        "print('-----------')\n",
        "\n",
        "\n",
        "# Remember: Period represents an interval in time, whereas Timestamp/DatetimeIndex represents a point in time."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BN3tWEOHbcXt"
      },
      "source": [
        "# As these are PeriodIndex object, we can print start and end times:\n",
        "print('Start time period at index 0: ', A[0].start_time) # if I do not specify the index this operation will be repeated for all the elements\n",
        "print('End time period at index 0: ', A[0].end_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgXY3iwMeKv_"
      },
      "source": [
        "###**1.3.3. Sequence of time intervals (deltas): `timedelta_range()`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0xEfrx6eUrq"
      },
      "source": [
        "A = pd.timedelta_range(start='10 days', periods=5)\n",
        "C = pd.timedelta_range('2 hours', freq='30T', periods=10)\n",
        "print(A)\n",
        "print(C)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lm5Vo2BM-ZeG"
      },
      "source": [
        "#**2. How to deal with tabular data**\n",
        "\n",
        "Besides manually creating DataFrames and Series objects, most of the times we will directly load in Python any file containing the data we want to analyse further. For example, in order to read CSV files in Pandas we have to call the *read_csv* method. Besides the name of the file, we add the *na_values* key argument to this method along with the character that represents \"non available data\" in the file. As most of the CSV files have a header with the names of the columns, the *usecols* parameter can be used to select which columns in the file will be used. This will also prevent to load all the columns from the file and thus to save memory/space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4j-yykO1OOhB"
      },
      "source": [
        "# These example data are part of the European Commission database. They represent government data, related in particular to\n",
        "# educational fundings by the member states.\n",
        "# In a delimiter-separated value file, as a CSV file, each line is a data record and each record consists of one (or more) fields, separated by the\n",
        "# delimiter character (usually a comma or semicolon).\n",
        "\n",
        "edu = pd.read_csv(DrivePath +'/Data/education_Data.csv', na_values=':',sep=';',usecols=['TIME','GEO','Value'])\n",
        "display(edu)\n",
        "print('-------')\n",
        "display(type(edu)) # DataFrame\n",
        "display(edu.dtypes) # To check the data type of each column\n",
        "\n",
        "# Remember: if we have loaded several columns and want to delete some of them:\n",
        "# edu.drop(columns=['GEO'],inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJafucsPAHo3"
      },
      "source": [
        "##**2.1 First check of the Data**\n",
        "\n",
        "To see how the data looks, we can use the *head()* method, which shows just the first five rows. if we put a number N as an argument, this will be the number of the first N rows that are listed. Similarly, *tail()* method shows the last N rows (five as defaults)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pKTZY5mW2md"
      },
      "source": [
        "edu.head()\n",
        "# edu.tail(7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVdYxe4xXHiz"
      },
      "source": [
        "*Columns*, *index* and *values* attributes can be used to retrieve information about the content of our DataFrame object.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yY-qsIoD0ihz"
      },
      "source": [
        "print('Columns:', edu.columns)\n",
        "print('')\n",
        "print('Indexes:', edu.index)\n",
        "print('')\n",
        "print('Values:', edu.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoWcFxdvAnOs"
      },
      "source": [
        "##**2.2 Data Selection**\n",
        "\n",
        "All the ways to access DataFrame objects we have seen before can be now applied to work on the data loaded from an Excel file or from websites, for example to select a single column or filter the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCyqY4igarzK"
      },
      "source": [
        "# Single Column -> result will be a Series data structure, not a DataFrame, because only one column is retrieved.\n",
        "T = edu['Value']\n",
        "display(T)\n",
        "display(type(T))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEAuo8DOa_CW"
      },
      "source": [
        "# Implicit slicing\n",
        "edu.iloc[10:14]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgTXXjASbWyA"
      },
      "source": [
        "# Explicit slicing\n",
        "edu.loc[10:14]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vRMAF8u0EZi"
      },
      "source": [
        "# Portion of the DataFrame\n",
        "edu.loc[0:4,'TIME':'GEO']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_3-9_dfc84r"
      },
      "source": [
        "# Filtering the DataFrame to select a subset of data (Boolean indexing)\n",
        "mask = edu['Value'] > 6.5\n",
        "edu[mask].head()\n",
        "# or better: edu[edu['Value'] > 6.5].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P56ZqPsor4PY"
      },
      "source": [
        "# Storing given information from the DataFrame in a NumPy Array\n",
        "data_array = np.array(edu['Value'].values) # To transform specific columns of DataFrame to NumPy Array\n",
        "display(data_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIPEdMx5AvLR"
      },
      "source": [
        "<u>Important Note</u>: Pandas uses the value NaN to represent missing values, which is a special floating-point value. A subtle feature of NaN values is that two NaN are never equal. Thus, the only safe way to tell whether or not a value is missing in a DataFrame is by using the *isnull()* function. Other useful functions are:\n",
        "1.   *notnull()*: Opposite of isnull()\n",
        "2.   *dropna()*: Return a filtered version of the data\n",
        "3.   *fillna()*: Return a copy of the data with missing values filled or imputed\n",
        "\n",
        "These functions can be used to filter rows with missing values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPsVQhY-hOVx"
      },
      "source": [
        "# To identify the NaN values\n",
        "null_elem = edu['Value'].isnull()\n",
        "display(edu[null_elem].head(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8uBdQsxm6g3"
      },
      "source": [
        "# To discard the NaN values\n",
        "edu_drop = edu.dropna() # To directly drop the entire rows with NaN\n",
        "edu_drop.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2Td51_Pobog"
      },
      "source": [
        "# If we aim at filling NaN values, we can choose 0 with the fillna(0) function\n",
        "edu_filled = edu.fillna(0)\n",
        "edu_filled.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternatively, we can specify a method to fill the values by propagating the previous or subsequent values (forward or backward-fill):\n",
        "# ffill: propagate last valid observation forward to next valid\n",
        "# bfill: use next valid observation to fill gap\n",
        "edu_filled = edu.fillna(method='bfill', axis=0)\n",
        "edu_filled.head(5)\n",
        "\n",
        "# axis can be used to define along which axis to fill missing values (0 or ‘index’, 1 or ‘columns’)\n",
        "#edu_filled = edu.fillna(method='ffill', axis=1)\n",
        "#edu_filled.head(5)"
      ],
      "metadata": {
        "id": "BonbvgAvJVQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqtcEWLCA1Gc"
      },
      "source": [
        "##**2.3 Sorting Data**\n",
        "Another important functionality to inspect data is to order them according to a given column. This can be achieved sorting any column, using *sort_values()*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrK6itUTBQxK"
      },
      "source": [
        "# Data sorted in descending order for 'Values' (i.e., from the largest to the smallest values):\n",
        "edu_ordered = edu.sort_values(by='Value', ascending=False, inplace=False)\n",
        "edu_ordered.head()\n",
        "\n",
        "# Note 1: the 'inplace = True' keyword means that the DataFrame will be overwritten, and hence no new DataFrame is returned.\n",
        "# Note 2: if 'ascending = True' the values are shown in ascending order"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2.4 Simple Descriptive Statistics**\n",
        "When loading the data in a dataframe, there is a convenience method `describe()` that computes several common aggregates for each column and returns the result. This can be a useful way to begin understanding the overall properties of a dataset."
      ],
      "metadata": {
        "id": "TDmsEzlxDCRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main aggregating functions we could use for Series and DataFrame objects is reported below:\n",
        "<figure>\n",
        "<center>\n",
        "<img src=https://drive.google.com/uc?id=1BuxNv3DbiljZSDSXCQvAx0E8xBdK8Bb0 width=\"350\"/>"
      ],
      "metadata": {
        "id": "7FyZ2NH0CeW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is applied on columns with numerical variables only\n",
        "display(edu.head(10))\n",
        "display(edu.dtypes)\n",
        "print('----------------')\n",
        "edu['Value'].describe() # To avoid results from TIME column which are not meaningful in this case (TIME is int, not yet a DatatimeIndex)"
      ],
      "metadata": {
        "id": "nAlTfOdtA4ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qqQ3hCGA8EK"
      },
      "source": [
        "##**2.5 Rearranging Data**\n",
        "\n",
        "In what we have seen so far, the indexes of our time series DataFrame created from the imported csv file have been simple row numbers (e.g., from 0 to 383) without much meaning. However, we can rearrange the data, redistributing the indexes and columns to better manipulate them and perform further operations, in an easier way. \\\\\n",
        "The *pivot_table()* function represents a further useful tool for such purpose. A pivot table takes simple column-wise data as input, and groups the entries into a two-dimensional table that provides a multidimensional summarization of the data. This *pivot_table()* function in Pandas requires to specify the columns to be used as new indexes, the new values and the new columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVzmnXwFGGh8"
      },
      "source": [
        "# Additional data organizations with pivot tables\n",
        "display(edu.head(4))\n",
        "display(edu.dtypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "piv_edu = pd.pivot_table(edu, values='Value',\n",
        "                        index=['GEO'], columns=['TIME'],aggfunc='median') # default aggfunc='mean'\n",
        "piv_edu.head(3) # It appears in ascending order\n",
        "\n",
        "# Note: if values are not specified, the pivot table will include the summary measures for all the variables with numerical values"
      ],
      "metadata": {
        "id": "9eekO3f8VwsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6slACGNYOTQM"
      },
      "source": [
        "# Accessing the data stored in the pivot table\n",
        "display(piv_edu[2003]) # To extract an entire Column - Alternative: piv_edu.loc[:,2003]\n",
        "# Note: I can not use '2003' (string) since the column names are Integer numbers, as confirmed by piv_edu.columns\n",
        "\n",
        "print('------------')\n",
        "display(piv_edu.loc[['Austria','Cyprus']]) # To extract all the values for two separate rows\n",
        "\n",
        "print('------------')\n",
        "display(piv_edu.iloc[0:2,0:3]) # To extract all the values for a part of the table (first two rows, three columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_MWuahfcY_3"
      },
      "source": [
        "# Important, as we will see better later:\n",
        "# when loading data from external sources, parse_dates allows to import ‘date’ as timestamps (DatetimeIndex),\n",
        "# while index_col specifies the index of our dataframe\n",
        "edu_up = pd.read_csv(DrivePath +'/Data/education_Data.csv', na_values=':',sep=';',usecols=['TIME','GEO','Value'],parse_dates=['TIME'],index_col='TIME')\n",
        "display(edu_up)\n",
        "edu_up.index # DatetimeIndex\n",
        "\n",
        "# If parse_dates is not included, index will be a simple series of Int numbers rather than a DatetimeIndex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Exercise 1:**\n",
        "\n",
        "Using the data stored in the \"sample_pivot.xlsx\" file, explore pivot tables.\n",
        "As first step, load the data (hint: function is `pd.read_excel()`, with parse_dates and index_col as for `pd.read_csv()`) setting the time information as index."
      ],
      "metadata": {
        "id": "OZfmc7CUGOhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data as required and visualise the DataFrame, including the types for the different columns\n",
        "\n"
      ],
      "metadata": {
        "id": "DTD0cri2_Mzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify whether NaN values are present in the \"Units\" columns and count how many they are\n",
        "\n",
        "\n",
        "# Fill the NaN values with the median value for that entire column\n",
        "\n"
      ],
      "metadata": {
        "id": "SB_dIP9_Dpi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pivot table from the filled DataFrame using Type and Region columns of interest, Units as values and mean as aggregating function\n"
      ],
      "metadata": {
        "id": "tLKutjL4Bbak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Create a pivot table from the filled DataFrame using Type and Region columns of interest.\n",
        " # For the aggregating function, use mean for Sales and sum for Units.\n"
      ],
      "metadata": {
        "id": "GWbt7WOmGxd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d405IAyVxMt5"
      },
      "source": [
        "##**2.6 Resampling**\n",
        "The process of converting a time series from one frequency to another is called *Resampling*. When higher frequency data are aggregated to lower frequency: **downsampling**; converting lower frequency to higher frequency: **upsampling**. \\\\\n",
        "This can be done using the `resample()` method, or `asfreq()` method. The primary difference between the two is that `resample()` is a data aggregation method, while `asfreq()` is a data selection. \\\\\n",
        "Some options for the resampling period:\n",
        "> W: weekly frequency \\\\\n",
        "> M: month end frequency \\\\\n",
        "> SM: semi-month end frequency (15th and end of month) \\\\\n",
        "> Q: quarter end frequency\n",
        "> BA: business year end frequency\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to investigate these aspects, we will load some stock price data from Yahoo finance API. This needs thre mandatory arguments in this order: \\\\\n",
        "> 1) Tickers (i.e., the name of the stock we want to load); \\\\\n",
        "> 2) Start date + End date or Period; \\\\\n",
        "> 3) Interval (i.e. the frequency/time frame we want to inspect the prices).\n",
        "\n",
        "Valid intervals are: 1m, 2m, 5m, 15m, 30m, 60m, 90m, 1h, 1d, 5d, 1wk, 1mo, 3mo"
      ],
      "metadata": {
        "id": "qZYhra3SxplX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4DmImaruzKj"
      },
      "source": [
        "!pip install yfinance\n",
        "import yfinance as yf\n",
        "\n",
        "# Option 1 -> specifying 'period'\n",
        "data = yf.download(tickers='AAPL', period='2d', interval='1h', progress=False, auto_adjust=False) # This returns a DataFrame\n",
        "display(data.head())\n",
        "\n",
        "print(data.shape)\n",
        "print(data.index) # DatetimeIndex object"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info() # To have some summary general information"
      ],
      "metadata": {
        "id": "vnwYrFVV5iKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Option 2 -> specifying start/end time\n",
        "data = yf.download(tickers='AAPL', start='2021-01-01', end='2021-12-31', interval='1wk', progress=False)\n",
        "data.head(7)"
      ],
      "metadata": {
        "id": "kyPz7Wp_8sT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTdrTVBLhzd3"
      },
      "source": [
        "# For accessing the data: different options with loc and iloc attributes as before\n",
        "print('Indexing (implicit):')\n",
        "print(data.iloc[0])\n",
        "print('----------')\n",
        "print('Indexing (explicit):')\n",
        "print(data.loc['2021-01-01'])\n",
        "print('----------')\n",
        "print('Slicing (explicit):')\n",
        "print(data.loc['2021-04-01':'2021-06-1'])\n",
        "print('----------')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qR283x9LDmAf"
      },
      "source": [
        "# Select a single column from the Dataframe storing the retrieved information\n",
        "data_closing = data['Adj Close']\n",
        "display(data_closing.head(12))\n",
        "\n",
        "print(data_closing.index) # DatetimeIndex object\n",
        "print(type(data_closing)) # Series\n",
        "print(data_closing.shape)\n",
        "\n",
        "data_closing.plot(); # First (basic!) plot for visualising the time series"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGJexbkQrYXj"
      },
      "source": [
        "# Resample() - Example\n",
        "Maximum = data_closing.resample('M').max()\n",
        "Maximum.plot(style=':')\n",
        "\n",
        "Mean = data_closing.resample('M').mean()\n",
        "Mean.plot(style='--')\n",
        "\n",
        "plt.legend(['Maximum', 'Mean'],loc='upper left');\n",
        "\n",
        "display('Original data (W freq):', data_closing.head(7))\n",
        "display('Resample data (M freq):', Maximum.head(7))\n",
        "\n",
        "# Note from the pd.resample() help:\n",
        "# The object must have a datetime-like index (DatetimeIndex, PeriodIndex, or TimedeltaIndex),\n",
        "# or the caller must pass the label of a datetime-like series/index to the \"on\" parameter.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZL0gIjPoatc"
      },
      "source": [
        "# Asfreq() - Example\n",
        "data_TEST = yf.download(tickers='TSLA', start='2021-01-01', end='2021-11-30', interval='1d', progress=False) # DataFrame\n",
        "display(data_TEST.head(6))\n",
        "data_5d = data_TEST['Adj Close'].asfreq('5d') # The values corresponding to any timesteps in the new index which were not present in the original index will be NaN\n",
        "print(data_5d)\n",
        "\n",
        "data_5d.plot(style='--')\n",
        "plt.legend(['5d freq'],loc='upper left');\n",
        "\n",
        "#data_5d_filled = data_TEST['Adj Close'].asfreq('5d', method='ffill') # To fill the NaN values\n",
        "#data_5d_filled.plot(style='-')\n",
        "#plt.legend(['5d freq filled'],loc='upper left');\n",
        "\n",
        "# I could combine the two, but it will only select specific values and fill the others with NaN\n",
        "#T = data_closing.resample('M').asfreq()\n",
        "#T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upsampling with Resample - In this case we have to add an interpolation step, rather than an aggregation function\n",
        "Up = data_closing.resample('3d').interpolate(method='linear')\n",
        "display(Up)\n",
        "\n",
        "fig,axs = plt.subplots(1,2,figsize=(15,5))\n",
        "data_closing.plot(ax=axs[0],marker = 'o', ms=3)\n",
        "Up.plot(ax=axs[1],marker = 'o', ms=3)"
      ],
      "metadata": {
        "id": "y9ZXMx9f2YdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwdC5c4bvVLt"
      },
      "source": [
        "###**Example 1 - Real Data**\n",
        "\n",
        "This dataset in the Temp_Prep_Data.csv file comprises information about the daily temperature (maximum in Fahrenheit) and total precipitation (inches) in July 2018 for Colorado. Data were provided by the National Oceanic and Atmospheric Administration. Here we will see why it is important to handle the date as DatetimeIndex objects rather than strings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjAE7ITUvUdF"
      },
      "source": [
        "# Loading data from csv\n",
        "temp_prep_data_orig = pd.read_csv(DrivePath +'/Data/Temp_Prep_Data.csv', na_values='-999',sep=',')\n",
        "display(temp_prep_data_orig.head(6))\n",
        "print(temp_prep_data_orig.dtypes) # date is object type\n",
        "print(temp_prep_data_orig.shape) # 31 rows, 3 columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rb9bjCFwOO3"
      },
      "source": [
        "# Initial visualisation\n",
        "fig, ax = plt.subplots(figsize=(10, 4));\n",
        "ax.plot(temp_prep_data_orig['date'],\n",
        "        temp_prep_data_orig['precip'],\n",
        "        color='green');\n",
        "\n",
        "ax.set(xlabel=\"Date\",\n",
        "       ylabel=\"Precipitation\",\n",
        "       title=\"Daily Total Precipitation\\nBoulder - Jul 2018 (Colorado)\");\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDi18-BRp_84"
      },
      "source": [
        "# NaN values could be filled\n",
        "temp_prep_data_filled = temp_prep_data_orig.fillna(method='bfill') # I could use other values as seen before\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 4));\n",
        "ax.plot(temp_prep_data_filled['date'],\n",
        "        temp_prep_data_filled['precip'],\n",
        "        color='green');\n",
        "\n",
        "ax.set(xlabel=\"Date\",\n",
        "       ylabel=\"Precipitation\",\n",
        "       title=\"Daily Total Precipitation\\nBoulder - Jul 2018 (Colorado)\");\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph05WHwjxtkO"
      },
      "source": [
        "If we look at the x-axis, Python gets stuck trying to plot the all of the date labels. Each value is read as a string, and it is difficult to try to fit all the values on the axis in an efficient way --> important to set the time information as datetime object during the import phase and possibly set this as index to easier all these processes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xTzEYxqxw1W"
      },
      "source": [
        "# Better way to import the data - 1\n",
        "temp_prep_data = pd.read_csv(DrivePath +'/Data/Temp_Prep_Data.csv', na_values='-999',sep=',',parse_dates=['date'])\n",
        "display(temp_prep_data.head(6))\n",
        "print(temp_prep_data.dtypes) # date is datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Better way to import the data - 2\n",
        "temp_prep_data = pd.read_csv(DrivePath +'/Data/Temp_Prep_Data.csv', na_values='-999',sep=',',parse_dates=['date'], index_col = 'date')\n",
        "display(temp_prep_data.head(6))\n",
        "print(temp_prep_data.dtypes)\n",
        "print(temp_prep_data.index) # datetime is now the index of the dataframe (DatetimeIndex)"
      ],
      "metadata": {
        "id": "n4HAxP68K3aF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resample with 3 days period\n",
        "Mean_3d = temp_prep_data['precip'].resample('3d').mean()\n",
        "display(Mean_3d)\n",
        "fig, ax = plt.subplots(figsize=(10, 4));\n",
        "ax.plot(Mean_3d,\n",
        "       color='green');\n",
        "\n",
        "ax.set(xlabel=\"Date\",\n",
        "       ylabel=\"Precipitation\",\n",
        "       title=\"Daily Total Precipitation\\nBoulder - Jul 2018 (Colorado)\");\n",
        "\n",
        "# Question:  what happens if I apply the resample function to the original \"temp_prep_data_orig\" data?"
      ],
      "metadata": {
        "id": "rYawyHuhLzIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHv76lwvuK4S"
      },
      "source": [
        "# Alternative with asfreq (note that the output is different)\n",
        "Asfreq_3d = temp_prep_data['precip'].asfreq('3d')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 6));\n",
        "ax.plot(Asfreq_3d,\n",
        "        color='green');\n",
        "\n",
        "ax.set(xlabel=\"Date\",\n",
        "       ylabel=\"Precipitation\",\n",
        "       title=\"Daily Total Precipitation\\nBoulder - Jul 2018 (Colorado)\");\n",
        "\n",
        "display(Asfreq_3d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Asfreq_3d)\n",
        "print('---------')\n",
        "display(Mean_3d)\n",
        "print('---------')\n",
        "display(temp_prep_data.head(20))"
      ],
      "metadata": {
        "id": "GlopEYihAy1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNnCS_FX0t1b"
      },
      "source": [
        "###**Example 2 - Real Data**\n",
        "\n",
        "The data we are going to analyses are related to earthquakes, in different periods and locations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UV2HYZw1jzG"
      },
      "source": [
        "# Import version 1\n",
        "earthquake_data = pd.read_csv(DrivePath +'/Data/Data_earthquakes.csv', na_values='',sep=',')\n",
        "display(earthquake_data.head())\n",
        "print(earthquake_data.dtypes) # datetime is object type\n",
        "#print(earthquake_data.shape) # 27 rows, 5 columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkC7WK5Z0ywl"
      },
      "source": [
        "# Import version 2 - compare this to the previous one\n",
        "earthquake_data = pd.read_csv(DrivePath +'/Data/Data_earthquakes.csv', na_values='',sep=',', parse_dates=['datetime'])\n",
        "display(earthquake_data.head())\n",
        "print(earthquake_data.dtypes) # datetime is datatime64 type\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl9qj24U2MeX"
      },
      "source": [
        "To split a column with date and time information into separate columns, `Series.dt` can be used to access the values of the series such as year, month, day etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PaKMVVR2OzG"
      },
      "source": [
        "# Splitting the date/time information\n",
        "earthquake_data['date'] = earthquake_data['datetime'].dt.date\n",
        "earthquake_data['time'] = earthquake_data['datetime'].dt.time\n",
        "earthquake_data['year'] = earthquake_data['datetime'].dt.year\n",
        "\n",
        "earthquake_data['month'] = earthquake_data['datetime'].dt.month\n",
        "earthquake_data['day'] = earthquake_data['datetime'].dt.day\n",
        "\n",
        "earthquake_data['hour'] = earthquake_data['datetime'].dt.hour\n",
        "earthquake_data['minute'] = earthquake_data['datetime'].dt.minute\n",
        "earthquake_data['second'] = earthquake_data['datetime'].dt.second\n",
        "\n",
        "# Drop the unnecessary columns (redundant)\n",
        "earthquake_data.drop(columns=['datetime'],inplace=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCQtAxvG3iFU"
      },
      "source": [
        "display(earthquake_data.head())\n",
        "print(earthquake_data.dtypes) # Date is an object\n",
        "print(earthquake_data['date'][0])\n",
        "\n",
        "# To convert Date to a Datetime object:\n",
        "earthquake_data['date'] = pd.to_datetime(earthquake_data['date'])\n",
        "print(earthquake_data.dtypes) # Date is a Datetime\n",
        "print(earthquake_data['date'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBN611aj52KY"
      },
      "source": [
        "If I want to compute the inverse operation, that is merge the individual columns for days, month, years, a Datetime object can be created using `pd.to_datetime` method seen before:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0K27kiD5oix"
      },
      "source": [
        "earthquake_data['new_datetime'] = pd.to_datetime(earthquake_data[[\"year\", \"month\", \"day\", \"hour\", \"minute\", \"second\"]])\n",
        "earthquake_data.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "earthquake_data.info()"
      ],
      "metadata": {
        "id": "g2QpchTZ3OCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsqgNfcUl3K9"
      },
      "source": [
        "##**Exercise 2 - Real Data**\n",
        "\n",
        "The data we will analyse in this exercise are measures of the global-scale temperature (global_temperature.csv), as provided by two different centers (from 1880 up to 2016)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUe6WebsO5ne"
      },
      "source": [
        "# 1. Load the data and set the \"Date\" column to a DatetimeIndex object.\n",
        "# Visualise the different types of data + the dimension of the dataframe.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Tt5RiGEGIW7"
      },
      "source": [
        "# 2. Type the following command:\n",
        "temperature_data.Mean[:350].plot();\n",
        "\n",
        "# Is this operation correct? What are we looking at?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "do8g8GrLGIW7"
      },
      "source": [
        "# 3. What the are steps you can apply to have a new table with Dates as index and two columns representing each one the temperatures for the two centers?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQZCqSqjGIW8"
      },
      "source": [
        "# 4. Visualise the temperature data from the two centers in a single figure\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Downsample the temperature data with year frequency, using mean as aggregator.\n",
        "# Visualise in a single figure the new temperature data for the two centers (yearly frequency)\n"
      ],
      "metadata": {
        "id": "BMLZmSc7GIW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89RHtoRtD8iH"
      },
      "source": [
        "#**3. Time Series Data Visualization in Python - Part 1**\n",
        "\n",
        "Data visualization is another essential task in all the different projects/domains, as it provides a clear idea of what the information means by reporting it visually through maps or graphs.\n",
        "Among the different possibilities for data visualization, Matplotlib library is one of the most well-known in Python. This is a multi-platform data visualization library built on NumPy arrays, which has been designed to work with the broader SciPy stack. It was conceived by John Hunter in 2002, originally as a patch for enabling interactive MATLAB-style plotting.\n",
        "While it has been largely used for years, people have been started to developing new packages (e.g., Seaborn, ggpy, HoloViews, Altair)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4WfGEKcFNVS"
      },
      "source": [
        "##**3.1. Time plots**\n",
        "\n",
        "For time series data, the obvious graph to start with is a time plot (as preliminary seen above). In this graph, the observations are plotted against the time of observation, with consecutive observations joined by straight lines."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "po4XpDwUFMBs"
      },
      "source": [
        "# General importing\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMkwgr8HGe8o"
      },
      "source": [
        "# Simple Time Plot\n",
        "y = np.linspace(0, 25, 2000)\n",
        "x =  pd.date_range('01-01-1850', periods=2000, freq='MS')\n",
        "fig = plt.figure()\n",
        "plt.plot(x, y**2, '-', label='time series');\n",
        "\n",
        "# To adjust the axis limits\n",
        "plt.xlim([x.min(), x.max()])\n",
        "plt.ylim(0, (y**2).max()+100);\n",
        "\n",
        "# To add labels and title\n",
        "plt.title(\"Example of a simple time plot\")\n",
        "plt.xlabel(\"Time [months]\")\n",
        "plt.ylabel(\"Number of observations\");\n",
        "\n",
        "# To add a legend\n",
        "plt.legend(frameon=True, loc='upper left');\n",
        "\n",
        "\n",
        "# plt.style.available # to check the available style that can be used"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Gvpp1PVGj1p"
      },
      "source": [
        "# General command for saving figures to files, directly in Google Drive\n",
        "fig.savefig(DrivePath +'/Data/my_time-plot.png',dpi=300)\n",
        "\n",
        "# To dowload on local computer\n",
        "#from google.colab import files\n",
        "#files.download( DrivePath +'/Data/my_figure.png')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yT99Vek9KmMJ"
      },
      "source": [
        "# EXTRA - Save all the figures directly in a pdf file\n",
        "!pip install fpdf\n",
        "from fpdf import FPDF\n",
        "from datetime import date\n",
        "\n",
        "pdf = FPDF(format='A4', unit='mm')\n",
        "pdf.add_page()\n",
        "pdf.set_font(\"Arial\", size=12)\n",
        "today = date.today()\n",
        "d2 = today.strftime(\"%B %d, %Y\")\n",
        "pdf.cell(200, 10, txt=\"Lesson 3 -  \" + str(d2), ln=1, align=\"C\")\n",
        "stringa = \"Simple time plot\"\n",
        "pdf.multi_cell(0,15,stringa)\n",
        "\n",
        "def add_image(image_path,pdf_file, w):\n",
        "    pdf_file.image(image_path, w=w)\n",
        "\n",
        "add_image(DrivePath +'/Data/my_time-plot.png',pdf, w=120)\n",
        "\n",
        "pdf.output(DrivePath + '/Data/'+'TEST_time-plot'+'.pdf');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH6KDyeL0X1q"
      },
      "source": [
        "For any scientific measurement, accounting for errors is equally important as accurately reporting the number itself. Indeed, when visualising data and results, showing these errors in an effective way allows to convey a more complete set of information.  \\\\\n",
        "A basic errorbar can be created with calling a simple Matplotlib function, named `errorbar()`. This is similar to the line plot, except that each data point comes with an errorbar to quantify uncertainty or variance present in each datum."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpgI56o31BDy"
      },
      "source": [
        "# Example 1\n",
        "from matplotlib.dates import DateFormatter\n",
        "\n",
        "date_form = DateFormatter(\"%Y\")\n",
        "np.random.seed(42)\n",
        "\n",
        "date_rng =  pd.date_range('01-01-1850', periods=100, freq='M')\n",
        "x = np.linspace(0, 10, 100)\n",
        "dy = 0.8\n",
        "y = np.sin(x) + dy * np.random.random(100) # to add noise to each point of a sinusoidal signal\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.errorbar(date_rng, y, yerr = dy, ecolor='lightgrey', elinewidth=2, capsize=2, fmt='o-'); # yerr specifies the error rate in the y direction\n",
        "ax.xaxis.set_major_formatter(date_form)\n",
        "ax.set_xlim(date_rng.min(), date_rng.max());\n",
        "ax.set_xlabel('Time')\n",
        "ax.set_ylabel('Values');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5u3qbjfH4l4e"
      },
      "source": [
        "# Example 2\n",
        "# In some cases, it might be useful to visualise the errorbar as shaded area across the time plot:\n",
        "mean_1 = np.array([10, 20, 30, 25, 32, 43])\n",
        "std_1 = np.array([2.2, 2.3, 1.2, 2.2, 1.8, 3.5])\n",
        "\n",
        "mean_2 = np.array([12, 22, 30, 13, 33, 39])\n",
        "std_2 = np.array([2.4, 1.3, 2.2, 1.2, 1.9, 3.5])\n",
        "\n",
        "date_form = DateFormatter(\"%b-%Y\")\n",
        "x = pd.date_range('01-2022', periods = len(mean_1), freq='MS')\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(x, mean_1, 'b-', label='Signal 1')\n",
        "ax.fill_between(x, mean_1 - std_1, mean_1 + std_1, color='b', alpha=0.2)\n",
        "ax.plot(x, mean_2, 'r-', label='Signal 2')\n",
        "ax.fill_between(x, mean_2 - std_2, mean_2 + std_2, color='r', alpha=0.2);\n",
        "ax.xaxis.set_major_formatter(date_form)\n",
        "ax.set_xlim(x.min(), xmax = x.max());\n",
        "ax.legend(frameon=False, loc='upper left', ncol=1);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSzA1RZp6Mrs"
      },
      "source": [
        "# Example 3\n",
        "# Representative fMRI time series (dataset already available in seaborn)\n",
        "#plt.style.use('seaborn')\n",
        "\n",
        "fmri = sns.load_dataset(\"fmri\")\n",
        "fmri_pivot = pd.pivot_table(fmri, index='timepoint',columns=['event','subject'],values='signal')\n",
        "display(fmri_pivot.tail(6))\n",
        "\n",
        "A = fmri_pivot['cue'].mean(axis=1) # A.shape = (19,)\n",
        "B = fmri_pivot['stim'].mean(axis=1)\n",
        "\n",
        "Astd = fmri_pivot['cue'].std(axis=1)\n",
        "Bstd = fmri_pivot['stim'].std(axis=1)\n",
        "\n",
        "x = list(range(0,len(A)))\n",
        "plt.plot(x,A, 'g-', label='Cue')\n",
        "plt.fill_between(x, A - Astd, A + Astd, color='g', alpha=0.2);\n",
        "plt.plot(x,B, 'b-', label='Stim')\n",
        "plt.fill_between(x,B - Bstd, B + Bstd, color='b', alpha=0.2);\n",
        "plt.legend(frameon=True, loc='upper right', ncol=1);\n",
        "plt.xlim([0,len(A)-1]);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeTgnEVP-k9i"
      },
      "source": [
        "# Alternative using the seaborn functionalities\n",
        "sns.lineplot(data=fmri, x='timepoint', y='signal',hue='event',ci='sd',estimator='mean');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTnKc6hDOHc9"
      },
      "source": [
        "##**3.2 Scatter plots**\n",
        "Time plots are useful for visualising individual time series. However, it is often necessary to explore relationships between multiple variables (time series in our case). Simple scatter plots can come into play for studying the relationship between two series by plotting one against the other.\n",
        "Instead of points being joined by line segments, here the points are represented individually with a dot, circle, or other shape. That is, each (x,y) coordinate pair is represented by a symbol."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wry_3idHSJht"
      },
      "source": [
        "# Example 1 on a real scenario: stock prices time series\n",
        "#!pip install yfinance\n",
        "#import yfinance as yf\n",
        "plt.style.use('seaborn')\n",
        "stocks = ['GOOG', 'AMZN']\n",
        "data_dw = yf.download(tickers=stocks, start = '2021-01-01', progress=False)\n",
        "data = data_dw['Adj Close']\n",
        "display(data.head(6))\n",
        "\n",
        "# Simple time plots\n",
        "from matplotlib import rcParams\n",
        "rcParams['figure.figsize'] = 12,6\n",
        "plt.plot(data.AMZN,label='Amazon')\n",
        "plt.plot(data.GOOG, label='Google')\n",
        "plt.grid(True, color='k', linestyle=':')\n",
        "plt.title(\"Amazon & Google Prices\")\n",
        "plt.xlabel(\"Date\")\n",
        "#plt.yticks([500, 1000,1500,2000,2500])\n",
        "plt.legend(frameon=False, loc='lower center', ncol=2);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6D1ZiC0U5XP"
      },
      "source": [
        "# Simple scatter plot -> The correlation does not seem high\n",
        "plt.scatter(data.GOOG, data.AMZN)\n",
        "plt.xlabel('Google')\n",
        "plt.ylabel('Amazon');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVD9rx1HVLjq"
      },
      "source": [
        "# Scatter plot on the difference\n",
        "returns = data.diff()\n",
        "display(returns.head(10))\n",
        "returns.dropna(inplace=True)\n",
        "\n",
        "rcParams['figure.figsize'] = 6,6\n",
        "plt.scatter(returns.GOOG, returns.AMZN, c = 'r', edgecolor='k')\n",
        "\n",
        "plt.axvline(0, c=(.5, .5, .5), ls='--')\n",
        "plt.axhline(0, c=(.5, .5, .5), ls='--')\n",
        "plt.xlabel('Google')\n",
        "plt.ylabel('Amazon')\n",
        "plt.xlim((-20,20))\n",
        "plt.ylim((-20,20));\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "er2_Jwd0ZcEm"
      },
      "source": [
        "# Example 2 - Iris dataset\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "print(iris.keys())\n",
        "print('Features:',iris.feature_names)\n",
        "print('Species:',iris.target_names)\n",
        "\n",
        "features = iris.data.T\n",
        "fig, ax = plt.subplots()\n",
        "rcParams['figure.figsize'] = 5,5\n",
        "\n",
        "scatter = ax.scatter(features[0], features[1], alpha=0.4, s=100*features[3], c=iris.target, cmap='viridis')\n",
        "ax.set_xlabel(iris.feature_names[0])\n",
        "ax.set_ylabel(iris.feature_names[1]);\n",
        "legend1 = ax.legend(*scatter.legend_elements(), loc = 'center', bbox_to_anchor=(1.1, 0.5), title='Species')\n",
        "ax.add_artist(legend1);\n",
        "# Note: legend_elements(prop ='sizes') produces a legend with a cross section of sizes from the scatter\n",
        "# legend2 = ax.legend(*scatter.legend_elements(prop='sizes'), loc= 'upper left', title='Sizes')\n",
        "\n",
        "# --> This type of visualization allows to simultaneously explore four different features of the data:\n",
        "# the (x, y) location of each point corresponds to the sepal length and width, the size of the point is related to the petal width,\n",
        "# and the color is related to the particular species of flower.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**3.2.1. Correlation**\n",
        "It is common to compute correlation coefficients to measure the strength of the linear relationship between two variables. The correlation between variables x and y is given by:\n",
        "<figure>\n",
        "<center>\n",
        "<img src=https://drive.google.com/uc?id=1bgp74cqZXyKwpQNDoZHsHdoLNojMqhPA\n",
        "width=\"350\"/>  \n"
      ],
      "metadata": {
        "id": "RtnFeestf2ys"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The value of r always lies between -1 and 1, with negative values indicating a negative relationship (data are anticorrelated) and positive values indicating a positive relationship."
      ],
      "metadata": {
        "id": "LGkFv5EshSXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple function to calculate the Pearson correlation value given two arrays of equal size\n",
        "x = np.array([-2, -1, 0, 1, 2])\n",
        "y = np.array([5, 1, 3, 2, 0])\n",
        "\n",
        "x = returns['AMZN']\n",
        "y = returns['GOOG']\n",
        "\n",
        "def correlation(x,y):\n",
        "  A = np.sum((x-np.mean(x))*(y-np.mean(y)))\n",
        "  B = np.sum((x-np.mean(x))**2) *np.sum((y-np.mean(y))**2)\n",
        "  corr = A/(B)**0.5\n",
        "  return(corr)\n",
        "\n",
        "A = correlation(x,y)\n",
        "print('Correlation value [Code] is:',A)\n"
      ],
      "metadata": {
        "id": "OODUJ-RVw_K4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple function to calculate the covariance value given two arrays of equal size\n",
        "def covariance(x,y):\n",
        "  A = np.sum((x-np.mean(x))*(y-np.mean(y)))\n",
        "  cov_val = A/(len(x)-1) # sample covariance\n",
        "  return(cov_val)\n",
        "\n",
        "A = covariance(x,y)\n",
        "print('Covariance value is:',A)"
      ],
      "metadata": {
        "id": "W3auXwHg0_1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: We can use the covariance value as numerator for the correlation equation:\n",
        "def correlation_v2(x,y):\n",
        "  A = covariance(x,y)\n",
        "  B = np.sum((x-np.mean(x))**2)/(len(x)-1) *np.sum((y-np.mean(y))**2)/(len(y)-1)\n",
        "  corr = A/(B)**0.5\n",
        "  return(corr)\n",
        "\n",
        "A = correlation_v2(x,y)\n",
        "print('Correlation value is:',A)"
      ],
      "metadata": {
        "id": "2ApN4ZDs3Ji_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To verify these values with Python available functions:\n",
        "\n",
        "# Pearson Correlation coefficient - Option 1\n",
        "my_corrcoef = np.corrcoef(x, y) # This returns a 2x2 matrix\n",
        "print('Correlation value [Python] is:', my_corrcoef[0,1])\n",
        "\n",
        "# Pearson Correlation coefficient - Option 2\n",
        "PC_val = stats.pearsonr(x,y) # This returns two values\n",
        "print('Correlation value [Python] is:',PC_val[0])\n",
        "print('The associated p-value is:',PC_val[1])\n",
        "\n",
        "COV_val = np.cov(x,y) # By default Python calculates the sample covariance. Returns a 2x2 matrix\n",
        "print('Covariance value [Python] is:',COV_val[0,1])"
      ],
      "metadata": {
        "id": "p5fYh3-I0wrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another measure of correlation we might use with time series data is *Spearman rank correlation* that is a nonparametric measure of rank correlation (statistical dependence between the rankings of two variables).\n",
        "While Pearson correlation assesses linear relationships, Spearman correlation assesses monotonic relationships, whether linear or not."
      ],
      "metadata": {
        "id": "8k9Khd_cyHWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The Spearman rank-order correlation coefficient is a nonparametric measure of the monotonicity of the relationship between two datasets.\n",
        "# Differently than Pearson correlation, this does not assume that both datasets are normally distributed.\n",
        "\n",
        "c = stats.spearmanr(x,y)\n",
        "print('Spearman Correlation value [Python] is:',c[0])\n",
        "print('The associated p-value is:',c[1])"
      ],
      "metadata": {
        "id": "VIXOyttNjjBx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}